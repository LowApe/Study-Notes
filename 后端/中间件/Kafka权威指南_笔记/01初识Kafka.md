# 初识 Kafka

Kafka 是一个流平台：在这个平台上可以发布和订阅数据流，并把它们保存起来、进行处理，这就是构建 Kafka 的初衷

内容总结：

- 数据为企业的发展提供动力
- 获取数据，分析数据，产生结果应用到实际场景

# 发布与订阅系统

数据的发送者不会**直接将消息发送给接受者**，这是发布与订阅系统的一个特点。发布者以某种方式**对消息进行分类**，接受者(订阅者)**订阅**它们，以便接收特定类型的消息。发布于订阅系统一般会有一个 broker ，也就是消息的中心点

## 如何开始

- 需要一个单一的集中式系统，它可以用来发布通用类型的数据，其规模可以随着公司业务的增长而增长



# Kafka 登场

Kafka 一般称为 “分布式提交日志” 或者 “分布式流平台”。文件系统或数据库提交日志用来提供所有事务的持久记录，通过重放这些日志可以重建系统的状态。

- Kafka 的数据是按照一定顺序持久化保存的，可以按需读取。此外，Kafka 的数据分布在整个系统里，具备**数据故障保障保护和性能伸缩**能力

## 消息和批次

Kafka 的**数据单元**被称为消息，也可以把消息单元看成是一个 "数据行"或者“记录”。**消息是字符数组组成，所以对于 Kafka 来说，消息里的数据没有特别的格式或含义**。

当消息以一种可控的方式写入不同的分区时，会用到键。最简单的例子就是为键生成一个一致性散列值，然后根据散列值对主题分区数进行取模，为消息选取分区。这样保证具有相同键的消息总是被写到相同的分区上

为了提高效率，消息被分批次写入 Kafka。**批次**就是一组消息，这些消息属于同一个主题和分区。如果每一个消息都单独发送到网络，会造成网络开销，分批次可以减少网络开销

## 模式

消息模式有很多可用的选项。像 JSON 和 XML 这些简单的系统，不仅易用，而且可读性好。但是它们缺乏强类型处理能力。Kafka 的许多开发者喜欢使用 **Apache Avro**，它最初是为 Hadoop 开放的一款**序列化框架**。Avro 提供了一种紧凑的序列化格式，模式和消息体是分开的，当模式发生变化时，不需要重新生成代码；他还支持强类型和模式进化，其版本即向前兼容，也向后兼容

- 数据格式的一致性对于 Kafka 来说很重要，它消除了消息读写操作之间的耦合性。如果读写操作紧密地耦合在一起，消息订阅者需要升级应用城西才能同时处理新旧两种数据格式



## 主题与分区

Kafka 的消息通过**主题**进行分类。主题就好比数据库的表，或者文件系统里的文件夹。主题可以被分为若干个分区，因此无法在整个主题范围内保证消息的顺序，但可以保证消息在**单个分区内的顺序**。

![image.png](http://ww1.sinaimg.cn/large/006rAlqhly1ga7j6d3gaxj30q80a8wgt.jpg)

> 如图：主题有 4 个分区，消息被追加写入每个分区的尾部。Kafka 通过分区来实现数据冗余和伸缩性。分区可以分布在不同的服务器上，也就是说，一个主题可以横跨多个服务器，以此来提供比单个服务器更强大的性能

我们通常会使用 **流** 这个词来描述 Kafka 这类系统的数据。很多时候，人们把一个主题看作一个流，不管它有多少个分区。**流是一组从生产者移动到消费者的数据**。当我们讨论流式处理时，一般都是

- Kafka Streams
- Apache Samza
- Storm 

这些框架以实时的方式处理消息，也就是所谓的流式处理。



## 生产者和消费者

Kafka 分为两种基本类型：

- 生产者
- 消费者

还有用于数据集成的 kafka Connect API 和用于流式处理的 Kafka Streams 。这些高级客户端 API 使用生产者和消费者作为内部组件，提供了高级功能

**生产者创建消息**在其他发布与订阅系统中，生产者可能被称为**发布者或写入者**。一般情况下，一个消息会被发布到一个特定的主题上。生产者在默认情况下把消息均衡地分布到主题的所有分区上，而不关心写到哪个分区。

> 在某些情况下，生产者会把消息直接写到指定分区。通过消息键和分区器来实现	

**消费者读取消息**。在其他发布与订阅系统中，消费者可能被称为**订阅者或读者**。消费者通过检查消息的偏移量来区分已经读过的消息。在给定的分区，每个消息的**偏移量🤔**都是唯一的。

**偏移量**是另一种元数据，它是一个不断递增的整数值，在创建消息时，Kafka 会把它添加到消息里。在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的消息偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或重启，它的读取状态不会丢失。

消费者是消费者群组的一部分，也就说，会有一个或多个消费者共同读取一个主题。**群组保证每个分区只能被一个消费者使用**

![image.png](http://ww1.sinaimg.cn/large/006rAlqhly1ga7jxn8jbzj30r60csmzx.jpg)

> 通过这种方式，消费者可以消费大量消费主题。而且，如果一个消费者失效，群组里的其他消费者可以接管失效消费者的工作

## broker 和集群

一个单独的 Kafka 服务器被称为 broker。broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。broker 消费者提供服务，对读取分区的请求作出响应，**返回已经提交到磁盘上的消息**

> 根据特定的硬件及其性能特性，单个 broker 可以轻松处理数千个分区以及每秒百万级的消息量

broker 是**集群**的组成部分。每个集群都有一个 broker 同时充当了**集群控制器**的角色(自动从集群的活跃成员中选举出来)控制器负责管理工作

- 将分区分配给 broker 和监控 broker。broker被称为分区的首领
- 一个分区可以分配给多个 broker，这个时候会发生分区复制。

![image.png](http://ww1.sinaimg.cn/large/006rAlqhly1ga7kg5xio3j30qq0fiwg8.jpg)

> 复制机制为分区提供了消息冗余，如果有一个 broker 失效，其他 broker 可以接管领导权。不过，相关的消费者和都要重新连接到**新的首领**

- **保留消息**是 Kafka 的一个重要特性。Kafka broker 默认的消息保留策略是这样的:要么保留一段时间(比如7天)，要么保留到消息达到一定大小的字节数(比如 1GB)
	主题可以配置自己的保留策略，可以将消息保留到不再使用它们为止。例如🌰：跟踪用户活动的数据可能需要保留几天，而应用程序的度量指标可能只需要保留几个小时

## 多集群

随着 Kafka 部署数量的增加，基于下面几个原因，最好使用多个集群。

- 数据类型分类
- 安全需求隔离
- 多数据中心(灾难恢复)

如果使用多个数据中，就需要在它们之间复制消息。

- kafka 的消息复制机制只能在单个集群里进行，不能在多个集群之间进行
- Kafka 提供了 MirrorMaker 的工具，可以用它来实现集群间的消息复制。MirrorMaker 的核心组件包括一个生产者和消费者，两者之间通过一个**队列**相连

![image.png](http://ww1.sinaimg.cn/large/006rAlqhly1ga7me4xmtwj30rk0i20wj.jpg)

# 为什么选择 Kafka

## 多个生产者

Kafka 可以无缝地支持多个生产者，不管客户端在使用单个主题还是多个主题。**所以它很适合用来从多个前端系统收集数据，并以统一的格式对外提供数据**。



🌰例如：一个包含了多个微服务的网站，可以为页面视图创建一个单独的主题，所以服务都以相同的消息格式向该主题写入数据。消费者应用程序会获得统一的页面视图，而无需协调来自不同生产者的数据流。



## 多个消费者

支持多个消费者从一个**单独的消息流**上读取数据，而且消费者之间互不影响。这与其他队列系统不同，其他队列系统的消息一旦被一个客户端读取，其他客户端就无法再读取它。多个消费者可以组成一个群组，它们共享一个消息流，并保证整个群组对每个给定的消息只处理一次

## 基于磁盘的数据存储

Kafka 不仅支持多个消费者，还允许消费者非实时地读取消息，这要归功于 Kafka 的数据保留特性。消息被提交到磁盘，根据设置的保留规则进行保存。每个主题可以单独的保留规则，满足不同消费者的需求。

> 消费者可以在进行应用程序维护时离线一小段时间，而无需担心消息丢失或堵塞在生产者端。消费者可以被关闭，但消息会继续保留在 Kafka 里。消费者可以从上次中断的地方继续处理消息

## 伸缩性

内容总结：

- 用户在开发阶段，可以先使用单个 broker，再扩展到包含 3 个 broker 的小型开发集群
- 提高汲取你的容错能力，需要配置较高的复制系数

## 高性能

通过横向扩展生产者、消费者和 broker，Kafka 可以轻松处理巨大的消息流。在处理大量数据的同时，它还能保证亚秒级的消息延迟



# 数据生态系统

内容总结：

- Kafka 为数据生态系统带来了循环系统，它在基础设施的各个组件之间传递消息，为所有客户端提供一致的接口。当与提供消息模式的系统集成时，生产者和消费者不再有紧密的耦合，也不需要在它们之间建立类型直连，🤔

	![image.png](http://ww1.sinaimg.cn/large/006rAlqhly1ga7n5g70ozj30qy0e2ac0.jpg)

## 使用场景

1. 活动跟踪
2. 传递消息
3. 度量指标和日志记录
4. 提交日志
5. 流处理



# 起源故事



# 开始 Kafka 之旅

开始使用 Kafka 来创建数据管道了。

